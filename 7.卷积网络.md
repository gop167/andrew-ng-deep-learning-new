# 计算机视觉

## 1. 边缘检测

<img src="assets/image-20210107122444277.png" alt="image-20210107122444277" style="zoom:50%;" />

### 1.1.1  垂直卷积filter/kernel

左边图像，两个不同维度

<img src="assets/clipboard-1609993614361.png" alt="img" style="zoom:50%;" />

30那部分所得的卷积运算确实是左边明右边亮的

dark to light -30

light to dark 

### 1.1.2 horizontal

图片足够大像10这种中间值过度就比较小，表明了一半在上一半在下

<img src="assets/clipboard-1609993696971.png" alt="img" style="zoom:50%;" />![img](assets/clipboard-1609993708292.png)

<img src="assets/clipboard-1609993696971.png" alt="img" style="zoom:50%;" />![img](assets/clipboard-1609993708292.png)

可以手写sobel，shcharr 滤波矩阵也可训练去更好地适应

## 2. 卷积核

### 2.1 padding

#### 2.1.1 目的

padding的目的：

1.怕图像缩小6*6->4*4

2.边缘点的信息利用率不高，和中间比少了很多次convolution操作

p的值可以是任意1，2这种

<img src="assets/clipboard-1609994159622.png" alt="img" style="zoom:50%;" />

### 2.2 卷积策略

valid：0padding

same：

ps：这里conv卷积核一般是奇数：

原因1.f偶数,p要是不对称的填充才能保证same convolution

2.有中心点能指出卷积核的位置

<img src="assets/clipboard-1609994294926.png" alt="img" style="zoom:50%;" />

### 2.3 卷积步长

<img src="assets/clipboard-1609994428199.png" alt="img" style="zoom:50%;" />

向下取整，no卷积核卷防止空了

ps：我们做的卷积严格来讲是cross correlation互相关

正真convolution有结合律和双重镜像，沿对角线flipping

<img src="assets/clipboard-1609994460962.png" alt="img" style="zoom:50%;" />

## 3.  三维卷积核

卷积核的长高可以不同但是channel必需相同；

3通道可以只关心红色的边界也可都关注

6*6*3 conv 3*3*3最后出来4*4单维度的

<img src="assets/clipboard-1609994970425.png" alt="img" style="zoom:50%;" />

<img src="assets/clipboard-1609994981100.png" alt="img" style="zoom: 67%;" />

检测不同的特征值nc‘==检测的特征数量，可检测水平和垂直的特征

## 4. 单层卷积网络

### 4.1 example

<img src="assets/clipboard-1609995232874.png" alt="img" style="zoom:50%;" />

2 filters

a[0]原始图像

做的工作==线性变换--俩relu激活层分别激活完了再叠加--4*4*2的图像 a[1]，2个特征映射

<img src="assets/clipboard-1609995269878.png" alt="img" style="zoom:50%;" />

不管原始图像多大都是280个参数去提取10个特征

<img src="assets/clipboard-1609995731779.png" alt="img" style="zoom:50%;" />

1.l层的输出大小根据l-1层的和l层的p,f解决

2.激活值的尺寸是这层卷过的

3.当执行批量梯度下降或者小批量梯度下降时，输出的A[l]如图，先索引/训练样例  然后是图片参数

4.weights--乘以nc[l]==bias数目

summary：一层的卷积层的原理以及激活值如何映射到下一层

## 5. 简单神经网络示例

<img src="assets/clipboard-1609996909519.png" alt="img" style="zoom:50%;" />

从一张图的输入到---输出假设的全过程尺寸不断缩小/提取特征越来愈多/最后排成一维向量logistic/softmax输出

<img src="assets/clipboard-1609996919709.png" alt="img" style="zoom:50%;" />

## 6.池化层

### 6.1 max pooling

<img src="assets/image-20210107134319897.png" alt="image-20210107134319897" style="zoom:50%;" />

<img src="assets/image-20210107134054219.png" alt="image-20210107134054219" style="zoom:50%;" />

梯度下降无需响应，f,s确定就是个固定运算

，如果在过滤器中提取到某个特征，那么保留最大值。右上角象限不存在这个特征，即使最大也很小值

### 6.2 average pooling

<img src="assets/image-20210107134517514.png" alt="image-20210107134517514" style="zoom:50%;" />

用在平均池化7**7***1000的表示层，在整个空间求平均值

<img src="assets/image-20210107134850704.png" alt="image-20210107134850704" style="zoom:50%;" />

加速学习，静态属性

## 7. 卷积神经网络示例

### 7.1 类LeNet-5

<img src="assets/image-20210107141218654.png" alt="image-20210107141218654" style="zoom:50%;" />

随着层数的增加nh，nw减少通道数量增加

自己别选超参数。

<img src="assets/image-20210107141627366.png" alt="image-20210107141627366" style="zoom:50%;" />

activate size不要减少太快！

## 8. 为什么用卷积

比起FC的优势

参数共享，稀疏连接

### 8.1. 参数少

<img src="assets/image-20210107142120788.png" alt="image-20210107142120788" style="zoom:50%;" />

#### 8.1.1 原因

<img src="assets/image-20210107142318735.png" alt="image-20210107142318735" style="zoom:50%;" />

平移不变属性，左移右移几个像素这张图片依然有非常相似的特征

<img src="assets/image-20210107142527506.png" alt="image-20210107142527506" style="zoom:50%;" />

参数少了减少了过拟合

<img src="assets/image-20210107142828005.png" alt="image-20210107142828005" style="zoom:50%;" />